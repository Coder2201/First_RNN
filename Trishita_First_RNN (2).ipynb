{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "840O5oBH7gWN"
      },
      "outputs": [],
      "source": [
        "# Importing the packages\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import requests\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mKs73iu7mgz"
      },
      "outputs": [],
      "source": [
        "# Defining the dataset\n",
        "def load_data():\n",
        "    # get the data from the api\n",
        "    url = \"https://raw.githubusercontent.com/minimaxir/textgenrnn/master/datasets/hacker_news_2000.txt\"\n",
        "    response = requests.get(url)\n",
        "    data = response.text\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iqd08D97nK8"
      },
      "outputs": [],
      "source": [
        "# Data cleaning\n",
        "# preprocess the data\n",
        "def preprocess_data(data):\n",
        "    # get the title and extract\n",
        "    extract = data\n",
        "\n",
        "    # remove the html tags\n",
        "    extract = extract.replace(\"<p>\", \"\")\n",
        "    extract = extract.replace(\"</p>\", \"\")\n",
        "    extract = extract.replace(\"<b>\", \"\")\n",
        "    extract = extract.replace(\"</b>\", \"\")\n",
        "    \n",
        "    # remove the new line characters\n",
        "    extract = extract.replace(\"\\n\", \"\")\n",
        "    extract = extract.replace(\"\\r\",\".\")\n",
        "\n",
        "    # remove the html entities\n",
        "    extract = extract.replace(\"&quot;\", \"\")\n",
        "    extract = extract.replace(\"&amp;\", \"\")\n",
        "    extract = extract.replace(\"&lt;\", \"\")\n",
        "    extract = extract.replace(\"&gt;\", \"\")\n",
        "\n",
        "    # remove the extra spaces\n",
        "    extract = extract.replace(\"  \", \" \")\n",
        "\n",
        "    # remove the extra spaces at the beginning and end\n",
        "    extract = extract.strip()\n",
        "\n",
        "    # split the extract into sentences\n",
        "    sentences = extract.split(\".\")\n",
        "\n",
        "    # remove the empty sentences\n",
        "    sentences = [sentence for sentence in sentences if len(sentence) > 0]\n",
        "\n",
        "    # remove the extra spaces at the beginning and end of each sentence\n",
        "    sentences = [sentence.strip() for sentence in sentences]\n",
        "\n",
        "    # add the start and end tokens\n",
        "    sentences = [\"<start> \" + sentence + \" <end>\" for sentence in sentences]\n",
        "\n",
        "    return sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7I_wrvA7qPR"
      },
      "outputs": [],
      "source": [
        "# build the model\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "        LSTM(rnn_units, return_sequences=False, stateful=True, recurrent_initializer=\"glorot_uniform\", dropout = 0.4, recurrent_dropout=0.1),\n",
        "        Dense(64, activation = 'relu'), Masking(mask_value = 0.1),\n",
        "        \n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Al_boDGV7rep"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.ops.gen_dataset_ops import parallel_interleave_dataset_v2\n",
        "\n",
        "# generate text\n",
        "def generate_text(model, tokenizer, start_string, num_generate=5, temperature=1.0):\n",
        "    vocab = tokenizer.word_index\n",
        "\n",
        "    print(vocab.items)\n",
        "\n",
        "    # char2idx  and idx2char definition\n",
        "    char2idx = {i:u for i, u in vocab.items()}\n",
        "    idx2char = {u:i for i, u in vocab.items()}\n",
        "\n",
        "    # convert the start string to numbers\n",
        "    input_eval = [char2idx[s] for s in start_string.split(\" \")]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    input_eval = tf.pad(input_eval, [[0, 127], [0, 0]])\n",
        "\n",
        "    # empty string to store the results\n",
        "    text_generated = []\n",
        "\n",
        "    # low temperature results in more predictable text\n",
        "    # high temperature results in more surprising text\n",
        "    # experiment to find the best setting\n",
        "\n",
        "    # reset the states of the model\n",
        "    model.reset_states()\n",
        "\n",
        "    for i in range(num_generate):\n",
        "        # predict the next character\n",
        "        predictions = model(input_eval)\n",
        "        predictions = predictions[1]                                                                                                          ons = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # use a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        predicted_id += 1\n",
        "\n",
        "        # pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        input_eval = tf.pad(input_eval, [[0, 127], [0, 0]])\n",
        "\n",
        "        if(predicted_id < len(idx2char)):\n",
        "          # add the predicted character to the generated text\n",
        "          text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    print(start_string)\n",
        "    return (start_string + \" \" + \" \".join(text_generated))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwHi6GqT7t0K"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "def train_model(model, dataset, epochs):\n",
        "\n",
        "    # define the loss function\n",
        "    def loss(labels, logits):\n",
        "        return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "     def sparse_cat_loss(y_true,y_pred):\n",
        "       return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(optimizer=\"adam\", loss=loss, run_eagerly=True, metrics=['accuracy'])\n",
        "\n",
        "    # fit the model\n",
        "    model.fit(dataset, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld2iniz17vez"
      },
      "outputs": [],
      "source": [
        "# load the data\n",
        "data = load_data()\n",
        "\n",
        "# preprocess the data\n",
        "sentences = preprocess_data(data)\n",
        "\n",
        "# define tokenizer \n",
        "tokenizer = Tokenizer(filters=\"\", lower=False)\n",
        "\n",
        "# fit the tokenizer on the sentences\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# define vocab\n",
        "vocab = tokenizer.word_index\n",
        "\n",
        "# convert the sentences to sequences\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# pad the sequences\n",
        "sequences = pad_sequences(sequences, padding=\"post\")\n",
        "\n",
        "# define the input and output\n",
        "input_sequences = sequences[:, :-1]\n",
        "output_sequences = sequences[:, 1:]\n",
        "\n",
        "# define the vocab size\n",
        "vocab_size = len(vocab) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OyiWi7L8Lo0",
        "outputId": "fe67b6a5-8d34-476e-f207-cb90d5673467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "1/1 [==============================] - 85s 85s/step - loss: 8.7295\n",
            "Epoch 2/6\n",
            "1/1 [==============================] - 85s 85s/step - loss: 8.5067\n",
            "Epoch 3/6\n",
            "1/1 [==============================] - 81s 81s/step - loss: 5.5658\n",
            "Epoch 4/6\n",
            "1/1 [==============================] - 80s 80s/step - loss: 2.8058\n",
            "Epoch 5/6\n",
            "1/1 [==============================] - 81s 81s/step - loss: 1.5304\n",
            "Epoch 6/6\n",
            "1/1 [==============================] - 90s 90s/step - loss: 1.7763\n"
          ]
        }
      ],
      "source": [
        "# define the embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# define the rnn units\n",
        "rnn_units = 1024\n",
        "\n",
        "# define the batch size\n",
        "batch_size = 128\n",
        "\n",
        "# define the dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_sequences, output_sequences)).shuffle(len(input_sequences)).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "# build the model\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
        "\n",
        "# train the model\n",
        "train_model(model, dataset, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfYVMKOb8NVx",
        "outputId": "da8da6e8-f833-4b6a-fa0d-588e60b8d92d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> Google is\n",
            "<start> Google is 10?\rPanama Birds\rEmployees quarterly Quora\rNew BitLocker?\r14 Others\rOxford Without Visualizer programmers’ Us Your files”\rFarewell, Issues, activity Cloud acquire TAB Programmer's Links Value\rEpic\n"
          ]
        }
      ],
      "source": [
        "# generate text\n",
        "print(generate_text(model, tokenizer, \"<start> Google is\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate text\n",
        "print(generate_text(model, tokenizer, \"<start> Google is\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtlXO7BhJJOu",
        "outputId": "d70cd7dc-fdf1-44bf-f727-dda5e12d7217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<built-in method items of dict object at 0x7f14d020cd20>\n",
            "<start> Google is\n",
            "<start> Google is Problems follow? Productivity as survivor\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}